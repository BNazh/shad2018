{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging with maximum entropy models (10 pts)\n",
    "\n",
    "In this task you will build a maximum entropy model for part-of-speech tagging. As the name suggests, our problem is all about converting a sequence of words into a sequence of part-of-speech tags. \n",
    "<img src=https://i.stack.imgur.com/6pdIT.png width=320>\n",
    "\n",
    "\n",
    "__Your man goal:__ implement the model from [the article you're given](W96-0213.pdf).\n",
    "\n",
    "Unlike previous tasks, this one gives you greater degree of freedom and less automated tests. We provide you with programming interface but nothing more.\n",
    "\n",
    "__A piece of advice:__ there's a lot of objects happening here. If you don't understand why some object is needed, find `def train` function and see how everything is linked together.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I: reading input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-31T19:19:18.382706Z",
     "start_time": "2018-10-31T19:19:18.377531Z"
    }
   },
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-31T19:19:18.635921Z",
     "start_time": "2018-10-31T19:19:18.626073Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data types:\n",
    "# Word: str\n",
    "# Sentence: list of str\n",
    "TaggedWord = collections.namedtuple('TaggedWord', ['text', 'tag'])\n",
    "# TaggedSentence: list of TaggedWord\n",
    "# Tags: list of TaggedWord\n",
    "# TagLattice: list of Tags\n",
    "\n",
    "def read_tagged_sentences(path):\n",
    "    \"\"\"\n",
    "    Read tagged sentences from CoNLL-U file and return array of TaggedSentence (array of lists of TaggedWord).\n",
    "    \"\"\"\n",
    "    tagged_sentences = []\n",
    "    tagged_sentence = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('#'):\n",
    "                continue\n",
    "            \n",
    "            if line != '':\n",
    "                id_, form, lemma, upos, xpos, feats, head, deprel, depth, misc = line.split('\\t')\n",
    "                tagged_sentence.append(TaggedWord(text=form, tag=upos))\n",
    "            else:\n",
    "                tagged_sentences.append(tagged_sentence)\n",
    "                tagged_sentence = []\n",
    "    if len(tagged_sentence) > 0:\n",
    "        tagged_sentences.append(tagged_sentence)\n",
    "    return tagged_sentences\n",
    "\n",
    "def write_tagged_sentence(tagged_sentence, f):\n",
    "    \"\"\"\n",
    "    Write tagged sentence in CoNLL-U format to file-like object f.\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    for id_, tagged_word in enumerate(tagged_sentence):\n",
    "        lines.append('{id_}\\t{form}\\t{lemma}\\t{upos}\\t{xpos}\\t{feats}\\t{head}\\t{deprel}\\t{depth}\\t{misc}'.format({\n",
    "            'id_': id_,\n",
    "            'form': tagged_word.text,\n",
    "            'lemma': '_',\n",
    "            'upos': tagged_word.tag,\n",
    "            'xpos': '_',\n",
    "            'feats': '_',\n",
    "            'deprel': '_',\n",
    "            'depth': '_',\n",
    "            'misc': '_',\n",
    "        }))\n",
    "    f.write('\\n'.join(lines))\n",
    "\n",
    "def read_tags(path):\n",
    "    \"\"\"\n",
    "    Read a list of possible tags from file and return the list.\n",
    "    \"\"\"\n",
    "    with open(path) as f:\n",
    "        return [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II: evaluation\n",
    "\n",
    "We want you to estimate tagging quality by a simple accuracy: a fraction of tag predictions that turned out to be correct - averaged over the entire training corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-31T19:19:19.401110Z",
     "start_time": "2018-10-31T19:19:19.397708Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data types:\n",
    "TaggingQuality = collections.namedtuple('TaggingQuality', ['acc'])\n",
    "\n",
    "def tagging_quality(ref, out):\n",
    "    \"\"\"\n",
    "    Compute tagging quality and return TaggingQuality object.\n",
    "    \"\"\"\n",
    "    nwords = 0\n",
    "    ncorrect = 0\n",
    "    import itertools\n",
    "    for ref_sentence, out_sentence in itertools.zip_longest(ref, out):\n",
    "        for ref_word, out_word in itertools.zip_longest():\n",
    "            if ref_word is None:\n",
    "                break\n",
    "            nwords += 1\n",
    "            if out_word is not None:\n",
    "                if ref_word.tag == out_word.tag:\n",
    "                    ncorrect += 1\n",
    "    return TaggingQuality(acc=ncorrect / nwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part III: Value and Update\n",
    "\n",
    "In order to implement two interlinked data structures: \n",
    "* __Value__ - a class that holds POS tagger's parameters. Basically an array of numbers\n",
    "* __Update__ - a class that stores updates for Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-31T19:19:20.720223Z",
     "start_time": "2018-10-31T19:19:20.663238Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-31T19:19:21.872594Z",
     "start_time": "2018-10-31T19:19:21.866167Z"
    }
   },
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, n):\n",
    "        \"\"\"\n",
    "        Dense object that holds parameters.\n",
    "        :param n: array length\n",
    "        \"\"\"\n",
    "        self.array = np.zeros(n)\n",
    "\n",
    "    def dot(self, update):\n",
    "        res = 0\n",
    "        for idx, value in update.map.items():\n",
    "            res += self.array[idx] * value\n",
    "        return res\n",
    "\n",
    "    def assign(self, other):\n",
    "        \"\"\"\n",
    "        self = other\n",
    "        other is Value.\n",
    "        \"\"\"\n",
    "        self.array = other.array.copy()\n",
    "\n",
    "    def assign_mul(self, coeff):\n",
    "        \"\"\"\n",
    "        self = self * coeff\n",
    "        coeff is float.\n",
    "        \"\"\"\n",
    "        self.array *= coeff\n",
    "\n",
    "    def assign_madd(self, x, coeff):\n",
    "        \"\"\"\n",
    "        self = self + x * coeff\n",
    "        x can be either Value or Update.\n",
    "        coeff is float.\n",
    "        \"\"\"\n",
    "        if isinstance(x, Value):\n",
    "            self.array += x.array * coeff\n",
    "        elif isinstance(x, Update):\n",
    "            for idx, value in x.map.items():\n",
    "                self.array[idx] += value*coeff\n",
    "        else:\n",
    "            raise ValueError('x must be Value or Update')\n",
    "\n",
    "\n",
    "class Update:\n",
    "    \"\"\"\n",
    "    Sparse object that holds an update of parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, positions=None, values=None):\n",
    "        \"\"\"\n",
    "        positions: array of int\n",
    "        values: array of float\n",
    "        \"\"\"\n",
    "        self.map = dict(zip(positions, values))\n",
    "\n",
    "    def assign_mul(self, coeff):\n",
    "        \"\"\"\n",
    "        self = self * coeff\n",
    "        coeff: float\n",
    "        \"\"\"\n",
    "        for idx in self.map:\n",
    "            self.map[idx] *= coeff\n",
    "\n",
    "    def assign_madd(self, update, coeff):\n",
    "        \"\"\"\n",
    "        self = self + update * coeff\n",
    "        coeff: float\n",
    "        \"\"\"\n",
    "        for idx, value in update.map.items():\n",
    "            if idx not in self.map:\n",
    "                self.map[idx] = value * coeff\n",
    "            else:\n",
    "                self.map[idx] += value * coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part IV: Maximum Entropy POS Tagger\n",
    "_step 1 - draw an oval; step 2 - draw the rest of the owl (c)_\n",
    "\n",
    "In this secion you will implement a simple linear model to predict POS tags.\n",
    "Make sure you [read the article](W96-0213.pdf) before you proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-31T19:19:25.449646Z",
     "start_time": "2018-10-31T19:19:25.445100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data Types:\n",
    "Features = Update\n",
    "Hypo = collections.namedtuple('Hypo', ['prev', 'pos', 'tagged_word', 'score'])\n",
    "# prev: previous Hypo\n",
    "# pos: position of word (0-based)\n",
    "# tagged_word: tagging of source_sentence[pos]\n",
    "# score: sum of scores over edges\n",
    "\n",
    "TaggerParams = collections.namedtuple('FeatureParams', [\n",
    "    'src_window',\n",
    "    'dst_order',\n",
    "    'max_suffix',\n",
    "    'beam_size',\n",
    "    'nparams'\n",
    "    ])\n",
    "\n",
    "import cityhash\n",
    "def h(x):\n",
    "    \"\"\"\n",
    "    Compute CityHash of any object.\n",
    "    Can be used to construct features.\n",
    "    \"\"\"\n",
    "    return cityhash.CityHash64(repr(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-31T19:19:28.782269Z",
     "start_time": "2018-10-31T19:19:28.779165Z"
    }
   },
   "outputs": [],
   "source": [
    "class LinearModel:\n",
    "    \"\"\"\n",
    "    A thing that computes score and gradient for given features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n):\n",
    "        self._params = Value(n)\n",
    "\n",
    "    def params(self):\n",
    "        return self._params\n",
    "\n",
    "    def score(self, features):\n",
    "        \"\"\"\n",
    "        features: Update\n",
    "        \"\"\"\n",
    "        return self._params.dot(features)\n",
    "\n",
    "    def gradient(self, features, score):\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-31T20:29:26.979628Z",
     "start_time": "2018-10-31T20:29:26.974882Z"
    }
   },
   "outputs": [],
   "source": [
    "class FeatureComputer:\n",
    "    def __init__(self, tagger_params, source_sentence):\n",
    "        self.tagger_params = tagger_params\n",
    "        self.source_sentence = source_sentence\n",
    "\n",
    "    def compute_features(self, hypo):\n",
    "        \"\"\"\n",
    "        Compute features for a given Hypo and return Update.\n",
    "        \"\"\"\n",
    "        feats = {}\n",
    "        \n",
    "        # w_{i-src_window}..w_{i+src_window+1}\n",
    "        w_i_formatter = 'w_{}={}'\n",
    "        for shift in range(-self.tagger_params.src_window, self.tagger_params.src_window + 1):\n",
    "            if 0 <= hypo.pos < len(self.source_sentence):\n",
    "                feat_hash = h(w_i_formatter.format(\n",
    "                    shift, self.source_sentence[hypo.pos]\n",
    "                )) % self.tagger_params.nparams\n",
    "                \n",
    "                self._add_to_feats(feat_hash, feats)\n",
    "                \n",
    "        # t_{i-dst_order}..t{i-1}\n",
    "        prev_tags = []\n",
    "        cur_hypo = hypo.prev\n",
    "        for i in range(dst_order):\n",
    "            if cur_hypo is not None:\n",
    "                prev_tags.append(cur_hypo.tagged_word.tag)\n",
    "                cur_hypo = cur_hypo.prev\n",
    "            else:\n",
    "                prev_tags.append('')\n",
    "        \n",
    "            feat_hash = h('_'.join(prev_tags)) % self.tagger_params.nparams\n",
    "            self._add_to_feats(feat_hash, feats)\n",
    "            \n",
    "        # w[-max_suffix:]..w[-1]\n",
    "        for i in range(-self.tagger_params.max_suffix, 0):\n",
    "            feat_hash = h('suffix_' + w[i:]) % self.tagger_params.nparams\n",
    "            self._add_to_feats(feat_hash, feats)\n",
    "            \n",
    "        return Update(positions=list(feats.keys()), values=list(feats.values()))\n",
    "        \n",
    "    def _add_to_feats(feat_hash, feats):\n",
    "        if feat_hash not in feats:\n",
    "            feats[feat_hash] = 1\n",
    "        else:\n",
    "            feats[feat_hash] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part V: Beam search\n",
    "\n",
    "We can find the most likely tagging approximately using Beam Search. As everything else, it comes with a separate interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-31T22:36:45.366595Z",
     "start_time": "2018-10-31T22:36:45.360490Z"
    }
   },
   "outputs": [],
   "source": [
    "class BeamSearchTask:\n",
    "    \"\"\"\n",
    "    An abstract beam search task. Can be used with beam_search() generic \n",
    "    function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tagger_params, source_sentence, model, tags):\n",
    "        self.tagger_params = tagger_params\n",
    "        self.source_sentence = source_sentence\n",
    "        self.model = model\n",
    "        self.tags = tags\n",
    "        self.feature_computer = FeatureComputer(tagger_params, source_sentence)\n",
    "\n",
    "    def total_num_steps(self):\n",
    "        \"\"\"\n",
    "        Number of hypotheses between beginning and end (number of words in\n",
    "        the sentence).\n",
    "        \"\"\"\n",
    "        return len(self.source_sentence)\n",
    "\n",
    "    def beam_size(self):\n",
    "        return self.tagger_params.beam_size\n",
    "\n",
    "    def expand(self, hypo):\n",
    "        \"\"\"\n",
    "        Given Hypo, return a list of its possible expansions.\n",
    "        'hypo' might be None -- return a list of initial hypos then.\n",
    "\n",
    "        Compute hypotheses' scores inside this function!\n",
    "        \"\"\"\n",
    "        idx = 0\n",
    "        if hypo is not None:\n",
    "            idx = hypo.pos + 1\n",
    "        text = self.source_sentence[idx]\n",
    "        \n",
    "        hypotheses = []\n",
    "        for tag in self.tags:\n",
    "            new_hypo = Hypo(hypo, idx, TaggedWord(text=text, tag=tag), 0)\n",
    "            # looks stupid btw\n",
    "            # TODO: rewrite\n",
    "            feats = self.feature_computer.compute_features(new_hypo)\n",
    "            score = self.model.score(feats)\n",
    "            new_hypo = Hypo(hypo, idx, TaggedWord(text=text, tag=tag), hypo.score + score)\n",
    "            hypotheses.append(new_hypo)\n",
    "        return hypotheses\n",
    "            \n",
    "\n",
    "    def recombo_hash(self, hypo):\n",
    "        \"\"\"\n",
    "        If two hypos have the same recombination hashes, they can be collapsed\n",
    "        together, leaving only the hypothesis with a better score.\n",
    "        \"\"\"\n",
    "        prev_tags = []\n",
    "        # +1 to include currect tag\n",
    "        for i in range(self.tagger_params.dst_order+1):\n",
    "            if hypo is not None:\n",
    "                prev_tags.append(hypo.tagged_word.tag)\n",
    "                hypo = hypo.prev\n",
    "        return h(prev_tags)\n",
    "\n",
    "\n",
    "def beam_search(beam_search_task):\n",
    "    \"\"\"\n",
    "    Return list of stacks.\n",
    "    Each stack contains several hypos, sorted by score in descending \n",
    "    order (i.e. better hypos first).\n",
    "    \"\"\"\n",
    "    stacks = []\n",
    "    for i in range(beam_search_task.total_num_steps):\n",
    "        if i != 0:\n",
    "            hypos = stacks[-1]\n",
    "        else:\n",
    "            hypos = [None]\n",
    "        \n",
    "        new_hypotheses = []\n",
    "        for hypo in hypos:\n",
    "            new_hypotheses.extend(beam_search_task.expand(hypo))\n",
    "        new_hypotheses = sorted(new_hypotheses, key=lambda x: x.score, reverse=True)\n",
    "        \n",
    "        stack = []\n",
    "        used_recombo_hashes = set()\n",
    "        for hypo in new_hypotheses:\n",
    "            if len(stack) >= beam_search_task.beam_size:\n",
    "                break\n",
    "            recombo_hash = beam_search_task.recombo_hash(hypo)\n",
    "            if recombo_hash not in used_recombo_hashes:\n",
    "                used_recombo_hashes.add(recombo_hash)\n",
    "                stack.append(hypo)\n",
    "        stacks.append(stack)\n",
    "    return stacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_sentences(dataset, <YOUR_PARAMS>):\n",
    "    \"\"\"\n",
    "    Main predict function.\n",
    "    Tags all sentences in dataset. Dataset is a list of TaggedSentence; while \n",
    "    tagging, ignore existing tags.\n",
    "    \"\"\"\n",
    "    <YOUR CODE>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part VI: Optimization objective and algorithm\n",
    "\n",
    "Once we defined our model and inference algorithm, we can define an optimization task: an object that computes loss function and its gradients w.r.t. model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizationTask:\n",
    "    \"\"\"\n",
    "    Optimization task that can be used with sgd().\n",
    "    \"\"\"\n",
    "\n",
    "    def params(self):\n",
    "        \"\"\"\n",
    "        Parameters which are optimized in this optimization task.\n",
    "        Return Value.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def loss_and_gradient(self, golden_sentence):\n",
    "        \"\"\"\n",
    "        Return (loss, gradient) on a specific example.\n",
    "\n",
    "        loss: float\n",
    "        gradient: Update\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class UnstructuredPerceptronOptimizationTask(OptimizationTask):\n",
    "    def __init__(self, ...):\n",
    "        <YOUR CODE>\n",
    "\n",
    "    def params(self):\n",
    "        <YOUR CODE>\n",
    "\n",
    "    def loss_and_gradient(self, golden_sentence):\n",
    "        <YOUR CODE>\n",
    "\n",
    "\n",
    "class StructuredPerceptronOptimizationTask(OptimizationTask):\n",
    "    def __init__(self, tagger_params, tags):\n",
    "        self.tagger_params = tagger_params\n",
    "        self.model = LinearModel(...)\n",
    "        self.tags = tags\n",
    "\n",
    "    def params(self):\n",
    "        return self.model.params()\n",
    "\n",
    "    def loss_and_gradient(self, golden_sentence):\n",
    "        # Do beam search.\n",
    "        beam_search_task = BeamSearchTask(\n",
    "            self.tagger_params, \n",
    "            [golden_tagged_word.text for golden_tagged_word in golden_sentence], \n",
    "            self.model, \n",
    "            self.tags\n",
    "            )\n",
    "        stacks = beam_search(beam_search_task)\n",
    "\n",
    "        # Compute chain of golden hypos (and their scores!).\n",
    "        golden_hypo = None\n",
    "        feature_computer = ...\n",
    "        for i in range(len(golden_sentence)):\n",
    "            new_golden_hypo = ...\n",
    "            golden_hypo = golden_hypo\n",
    "\n",
    "        # Find where to update.\n",
    "        golden_head = <YOUR CODE>\n",
    "        rival_head = <YOUR CODE>\n",
    "\n",
    "        # Compute gradient.\n",
    "        grad = Update()\n",
    "        while golden_head and rival_head:\n",
    "            rival_features = feature_computer.compute_features(rival_head)\n",
    "            grad.assign_madd(self.model.gradient(rival_features, score=None), 1)\n",
    "\n",
    "            golden_features = feature_computer.compute_features(golden_head)\n",
    "            grad.assign_madd(self.model.gradient(golden_features, score=None), -1)\n",
    "\n",
    "\n",
    "            golden_head = golden_head.prev\n",
    "            rival_head = rival_head.prev\n",
    "\n",
    "        return grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part VII: optimizer\n",
    "\n",
    "By this point we can define a model with parameters $\\theta$ and a problem that computes gradients $ \\partial L \\over \\partial \\theta $ w.r.t. model parameters.\n",
    "\n",
    "Optimization is performed by gradient descent: $ \\theta := \\theta - \\alpha {\\partial L \\over \\partial \\theta} $\n",
    "\n",
    "In order to speed up training, we use stochastic gradient descent that operates on minibatches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGDParams = collections.namedtuple('SGDParams', [\n",
    "    'epochs',\n",
    "    'learning_rate',\n",
    "    'minibatch_size',\n",
    "    'average' # bool or int\n",
    "    ])\n",
    "\n",
    "\n",
    "def make_batches(dataset, minibatch_size):\n",
    "    \"\"\"\n",
    "    Make list of batches from a list of examples.\n",
    "    \"\"\"\n",
    "    <YOUR CODE>\n",
    "\n",
    "\n",
    "def sgd(sgd_params, optimization_task, dataset, after_each_epoch_fn):\n",
    "    \"\"\"\n",
    "    Run (averaged) SGD on a generic optimization task. Modify optimization\n",
    "    task's parameters.\n",
    "\n",
    "    After each epoch (and also before and after the whole training),\n",
    "    run after_each_epoch_fn().\n",
    "    \"\"\"\n",
    "    <YOUR CODE>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part VIII: Training loop\n",
    "\n",
    "The train function combines everthing you used below to get new "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    tags='./data/tags',\n",
    "    train_dataset='./data/en-ud-train.conllu',\n",
    "    dev_dataset='./data/en-ud-dev.conllu',\n",
    "    model='./model.npz',\n",
    "    \n",
    "    sgd_epochs=15,\n",
    "    sgd_learning_rate=0.01,\n",
    "    sgd_minibatch_size=32,\n",
    "    \n",
    "    # Number of context tags in output tagging to use for features\n",
    "    tagger_src_window=2,\n",
    "    \n",
    "    # Number of context tags in output tagging to use for features\n",
    "    tagger_dst_order=3,\n",
    "    \n",
    "    # Maximal number of prefix/suffix letters to use for features\n",
    "    tagger_max_suffix=4,\n",
    "    \n",
    "    # Width for beam search (0 means unstructured)\n",
    "    beam_size=1,\n",
    "    \n",
    "    # Parameter vector size (for hashing)\n",
    "    nparams= 2 * 22,\n",
    "):\n",
    "    \"\"\" Train a pos-tagger model and save it's parameters to :model: \"\"\"\n",
    "\n",
    "    # Beam size.\n",
    "    optimization_task_cls = StructuredPerceptronOptimizationTask\n",
    "    if beam_size == 0:\n",
    "        beam_size = 1\n",
    "        optimization_task_cls = UnstructuredPerceptronOptimizationTask\n",
    "\n",
    "    # Parse cmdargs.\n",
    "    tags = read_tags(cmdargs.tags)\n",
    "    train_dataset = read_tagged_sentences(train_dataset)\n",
    "    dev_dataset = read_tagged_sentences(dev_dataset)\n",
    "    params = None\n",
    "    if os.path.exists(cmdargs.model):\n",
    "        params = pickle.load(open(cmdargs.model, 'rb'))\n",
    "    sgd_params = SGDParams(\n",
    "        epochs=sgd_epochs,\n",
    "        learning_rate=sgd_learning_rate,\n",
    "        minibatch_size=sgd_minibatch_size,\n",
    "        average=sgd_average\n",
    "        )\n",
    "    tagger_params = TaggerParams(\n",
    "        src_window=tagger_src_window,\n",
    "        dst_order=tagger_dst_order,\n",
    "        max_suffix=tagger_max_suffix,\n",
    "        beam_size=beam_size,\n",
    "        nparams=nparams\n",
    "        )\n",
    "\n",
    "    # Load optimization task\n",
    "    optimization_task = optimization_task_cls(...)\n",
    "    if params is not None:\n",
    "        print('\\n\\nLoading parameters from %s\\n\\n' % cmdargs.model)\n",
    "        optimization_task.params().assign(params)\n",
    "\n",
    "    # Validation.\n",
    "    def after_each_epoch_fn():\n",
    "        model = LinearModel(cmdargs.nparams)\n",
    "        model.params().assign(optimization_task.params())\n",
    "        tagged_sentences = tag_sentences(dev_dataset, <YOUR_PARAMS>)\n",
    "        q = pprint.pformat(tagging_quality(out=tagged_sentences, ref=dev_dataset))\n",
    "        print()\n",
    "        print(q)\n",
    "        print()\n",
    "\n",
    "        # Save parameters.\n",
    "        print('\\n\\nSaving parameters to %s\\n\\n' % cmdargs.model)\n",
    "        pickle.dump(optimization_task.params(), open(cmdargs.model, 'wb'))\n",
    "\n",
    "    # Run SGD.\n",
    "    sgd(sgd_params, optimization_task, train_dataset, after_each_epoch_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a model with default params\n",
    "train(model='./default_model.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part IX: Evaluate the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-c5d1753b0c3f>, line 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-c5d1753b0c3f>\"\u001b[0;36m, line \u001b[0;32m30\u001b[0m\n\u001b[0;31m    tagged_sentences = tag_sentences(dataset, <YOUR_PARAMS>)\u001b[0m\n\u001b[0m                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def test(\n",
    "    tags='./data/tags',\n",
    "    dataset='./data/en-ud-dev.conllu',\n",
    "    model='./model.npz',\n",
    "    \n",
    "    # model and inference params; see train for their description\n",
    "    tagger_src_window=2,\n",
    "    tagger_dst_order=3,\n",
    "    tagger_max_suffix=4,\n",
    "    beam_size=1,\n",
    "):\n",
    "\n",
    "\n",
    "    tags = read_tags(tags)\n",
    "    dataset = read_tagged_sentences(dataset)\n",
    "    params = pickle.load(open(model, 'rb'))\n",
    "    tagger_params = TaggerParams(\n",
    "        src_window=tagger_src_window,\n",
    "        dst_order=tagger_dst_order,\n",
    "        max_suffix=tagger_max_suffix,\n",
    "        beam_size=beam_size,\n",
    "        nparams=0\n",
    "        )\n",
    "\n",
    "    # Load model.\n",
    "    model = LinearModel(params.values.shape[0])\n",
    "    model.params().assign(params)\n",
    "\n",
    "    # Tag all sentences.\n",
    "    tagged_sentences = tag_sentences(dataset, <YOUR_PARAMS>)\n",
    "\n",
    "    # Write tagged sentences.\n",
    "    for tagged_sentence in tagged_sentences:\n",
    "        write_tagged_sentence(tagged_sentence, sys.stdout)\n",
    "\n",
    "    # Measure and print quality.\n",
    "    q = pprint.pformat(tagging_quality(out=tagged_sentences, ref=dataset))\n",
    "    print(q, file=sys.stderr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test \n",
    "test(model='./default_model.npz')\n",
    "\n",
    "# sanity chec: accuracy > 90%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part X: play with it\n",
    "\n",
    "_This part is optional_\n",
    "\n",
    "Once you've built something, it's only natural to test the limits of your contraption.\n",
    "\n",
    "At minumum, we want you to find out how default model accuracy depends on __beam size__\n",
    "\n",
    "To get maximum points, your model should get final quality >= 93% \n",
    "\n",
    "Any further analysis is welcome, as always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR CODE>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
